<!doctype html><html lang=en><head><meta name=generator content="Hugo 0.75.1"><meta charset=utf-8><title>How Deep Are scikit-learn's Histogram-based Gradient Boosted Trees?</title><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="black-translucent"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><link rel=stylesheet href=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/css/reset.css><link rel=stylesheet href=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/css/reveal.css><link rel=stylesheet href=/2020-richmond-ds-meetup-gradient-boosting/_.min.8dd66bfde90c8f066cab2c5dcd63eb6d12857a9d0c33e2f2f97e57cf9db39c36.css id=theme><link rel=stylesheet href=/2020-richmond-ds-meetup-gradient-boosting/highlight-js/monokai-sublime.min.css></head><body><div class=reveal><div class=slides><section><h1 id=how-deep-are-scikit-learns-histogram-based-gradient-boosted-trees>How Deep Are scikit-learn&rsquo;s Histogram-based Gradient Boosted Trees?</h1><h1 id=heading>ğŸ—»ğŸš€ğŸ„</h1><p>Thomas J. Fan</p><p><p class=social-author>@thomasjpfan</p><a href=https://www.github.com/thomasjpfan target=_blank><span class="icon icon-github icon-left"></span></a><a href=https://www.twitter.com/thomasjpfan target=_blank><span class="icon icon-twitter"></span></a><div></div><a class=this-talk-link , href=https://github.com/thomasjpfan/2020-richmond-ds-meetup-gradient-boosting target=_blank>This talk on Github: thomasjpfan/2020-richmond-ds-meetup-gradient-boosting</a></p></section><section><section data-shortcode-section><h1 id=supervised-learning->Supervised Learning ğŸ“–</h1><p>$$
y = f(X)
$$</p><ul><li>$X$ of shape <code>(n_samples, n_features)</code></li><li>$y$ of shape <code>(n_samples,)</code></li></ul></section><section><h1 id=scikit-learn-api->Scikit-learn API ğŸ› </h1><pre><code class=language-python>from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

clf = HistGradientBoostingClassifier()

clf.fit(X, y)

clf.predict(X)
</code></pre></section></section><section><section data-shortcode-section><h1 id=histgradient-boosting->HistGradient-<em>Boosting</em> ğŸš€</h1></section><section><h1 id=boosting->Boosting ğŸš€</h1><p>$$
f(X) = h_0(X) + h_1(X) + h_2(X) + &mldr;
$$</p><p>$$
f(X) = \sum_i h_i(X)
$$</p></section><section><h1 id=hist-gradient-boosting->Hist-<em>Gradient</em>-Boosting ğŸ—»</h1></section><section><h1 id=gradient->Gradient ğŸ—»</h1><div class=g><div class=g-1><h2 id=regression>Regression</h2><ul><li><code>least_squares</code></li><li><code>least_absolute_deviation</code></li><li><code>poisson</code></li></ul></div><div class=g-1><h2 id=classificaiton>Classificaiton</h2><ul><li><code>binary_crossentropy</code></li><li><code>categorical_crossentropy</code></li><li><code>auto</code></li></ul></div></div></section><section><h1 id=loss-function---least_squares>Loss Function - <code>least_squares</code></h1><p>$$
L(y, f(X)) = \frac{1}{2}||y - f(X)||^2
$$</p><div class=g><div class=g-1><h2 id=gradient>Gradient</h2><p>$$
\nabla L(y, f(X)) = -(y - f(X))
$$</p></div><div class=g-1><h2 id=hessian>Hessian</h2><p>$$
\nabla^2 L(y, f(X)) = 1
$$</p></div></div></section><section><h1 id=gradient-boosting->Gradient Boosting ğŸ—»ğŸš€</h1><ul><li>Initial Condition</li></ul><p>$$
f_0(X) = C
$$</p><ul><li>Recursive Condition</li></ul><p>$$
f_{m+1}(X) = f_{m}(X) - \eta \nabla L(y, f_{m}(X))
$$</p><p>where $\eta$ is the learning rate</p></section><section><h1 id=gradient-boosting----least_squares>Gradient Boosting ğŸ‚ - <code>least_squares</code></h1><ul><li>Plugging in gradient for least_square</li></ul><p>$$
f_{m+1}(X) = f_{m}(X) + \eta(y - f_{m}(X))
$$</p><ul><li>Letting $h_{m}(X)=(y - f_{m}(X))$</li></ul><p>$$
f_{m+1}(X) = f_{m}(X) + \eta h_{m}(X)
$$</p><ul><li>We need to learn $h_{m}(X)$!</li><li>For the next example, let $\eta=1$</li></ul></section><section><h1 id=gradient-boosting----example-part-1>Gradient Boosting ğŸ‚ - (Example, part 1)</h1><p>$$
f_0(X) = C
$$</p><figure><img src=images/gb-p1.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-2>Gradient Boosting ğŸ‚ - (Example, part 2)</h1><figure><img src=images/gb-p2.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-3>Gradient Boosting ğŸ‚ - (Example, part 3)</h1><figure><img src=images/gb-p3.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-4>Gradient Boosting ğŸ‚ - (Example, part 4)</h1><p>$$
f_{m+1}(X) = f_{m}(X) + h_{m}(X)
$$</p><figure><img src=images/gb-p4.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-5>Gradient Boosting ğŸ‚ - (Example, part 5)</h1><figure><img src=images/gb-p5.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-6>Gradient Boosting ğŸ‚ - (Example, part 6)</h1><figure><img src=images/gb-p6.png height=500px></figure></section><section><h1 id=gradient-boosting----example-part-7>Gradient Boosting ğŸ‚ - (Example, part 7)</h1><figure><img src=images/gb-p7.png height=500px></figure></section><section><h1 id=gradient-boosting--1>Gradient Boosting ğŸ‚</h1><p>With two iterations of boosting:</p><p>$$
f(X) = C + h_0(X) + h_1(X)
$$</p><h2 id=prediction>Prediction</h2><p>For example, with $X=40$</p><p>$$
f(40) = 78 + h_0(40) + h_1(40)
$$</p></section></section><section><section data-shortcode-section><h1 id=how-to-learn-h_mx>How to learn $h_m(X)$?</h1></section><section><h1 id=heading>ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„</h1><h1 id=heading-1>ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„</h1><h1 id=heading-2>ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„</h1><h1 id=heading-3>ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„</h1><h1 id=heading-4>ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„ğŸ„</h1></section><section><h1 id=tree-growing->Tree Growing ğŸŒ²</h1><div class=g><div class=g-1><ol><li>For every feature<ol><li>Sort feature</li><li>For every split point<ol><li><strong>Evaluate split</strong></li></ol></li></ol></li><li>Pick best split</li></ol></div><div class=g-1><p><img src=images/tree-growing-1.png alt></p></div></div></section><section><h1 id=how-to-evaluate-split>How to evaluate split?</h1><h2 id=least_square><code>least_square</code></h2><ul><li>Recall Loss, Gradient, Hessian</li></ul><p>$$
L(y, f(X)) = \frac{1}{2}||y - f(X)||^2
$$</p><p>$$
G = \nabla L(y, f(X)) = -(y - f(X))
$$</p><p>$$
H = \nabla^2 L(y, f(X)) = 1
$$</p></section><section><h1 id=how-to-evaluate-split-1>How to evaluate split?</h1><p>Maximize the Gain!</p><p>$$
Gain = \dfrac{1}{2}\left[\dfrac{G_L^2}{H_L+\lambda} + \dfrac{G_R^2}{H_R + \lambda} - \dfrac{(G_L+G_R)^2}{H_L+H_R+\lambda}\right]
$$</p><p>default $\lambda$: <code>l2_regularization=0</code></p></section><section><h1 id=tree-growing--1>Tree Growing ğŸ„</h1><div class=g><div class=g-1><h2 id=are-we-done>Are we done?</h2><ol><li>For every feature<ol><li>Sort feature</li><li>For every split point<ol><li>Evaluate split</li></ol></li></ol></li><li>Pick best split</li></ol></div><div class=g-1><p><img src=images/tree-growing-2.png alt></p></div></div></section><section><h1 id=tree-growing--2>Tree Growing ğŸ„</h1><div class=g><div class=g-1><h2 id=are-we-done>Are we done?</h2><ol><li>For every feature<ol><li>Sort feature - <em><strong>O(nlog(n))</strong></em></li><li>For every split point - <em><strong>O(n)</strong></em><ol><li>Evaluate split</li></ol></li></ol></li><li>Pick best split</li></ol></div><div class=g-1><p><img src=images/tree-growing-2.png alt></p></div></div></section></section><section><section data-shortcode-section><h1 id=hist-gradientboosting><em>Hist</em>-GradientBoosting</h1></section><section><h1 id=binning->Binning! ğŸ—‘</h1><figure><img src=images/binning101.png></figure></section><section><h1 id=binning--1>Binning! ğŸ—‘</h1><pre><code class=language-py># Original data
[-0.752,  2.7042,  1.3919,  0.5091, -2.0636,
 -2.064, -2.6514,  2.1977,  0.6007,  1.2487, ...]

# Binned data
[4, 9, 7, 6, 2, 1, 0, 8, 6, 7, ...]
</code></pre></section><section><h1 id=histograms->Histograms! ğŸ“Š</h1><figure><img src=images/binned-gradient-hess.png height=500px></figure></section><section><h1 id=histograms--1>Histograms! ğŸ“Š</h1><div class=g><div class=g-1><h2 id=overview>Overview</h2><ol><li>For every feature<ol><li>Build histogram <em><strong>O(n)</strong></em></li><li>For every split point - <em><strong>O(n_bins)</strong></em><ol><li>Evaluate split</li></ol></li></ol></li><li>Pick best split</li></ol></div><div class=g-1><p><img src=images/gradient-split-points.png alt></p></div></div></section><section><h1 id=one-more-trick->One More Trick ğŸ©</h1><figure><img src=images/histogram_subtraction.png></figure></section><section><h1 id=trees--h_mx->Trees = $h_m(X)$ ğŸ„</h1><p>$$
f(X) = C + \sum h_{m}(X)
$$</p></section></section><section><section data-shortcode-section><h1 id=overview-of-algorithm->Overview of Algorithm ğŸ‘€</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>Find best splits</li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=implementation->Implementation? ğŸ¤”</h1><ul><li>Pure Python?</li><li>Numpy?</li><li>Cython?</li><li>Cython + OpenMP!</li></ul></section><section><h1 id=openmp-bin-data->OpenMP! Bin data ğŸ—‘</h1><ol><li><em><strong>Bin data</strong></em></li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>Find best splits by building histograms</li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=openmp-bin-data--1>OpenMP! Bin data ğŸ—‘</h1><pre><code class=language-python data-line-numbers=|1>for i in range(n_samples):
    left, right = 0, binning_thresholds.shape[0]
    while left &lt; right:
        middle = left + (right - left - 1) // 2
        if data[i] &lt;= binning_thresholds[middle]:
            right = middle
        else:
            left = middle + 1
    binned[i] = left
</code></pre></section><section><h1 id=openmp-bin-data--2>OpenMP! Bin data ğŸ—‘</h1><pre><code class=language-python data-line-numbers=|2># sklearn/ensemble/_hist_gradient_boosting/_binning.pyx
for i in prange(n_samples, schedule='static', nogil=True):
    left, right = 0, binning_thresholds.shape[0]
    while left &lt; right:
        middle = left + (right - left - 1) // 2
        if data[i] &lt;= binning_thresholds[middle]:
            right = middle
        else:
            left = middle + 1
    binned[i] = left
</code></pre></section><section><h1 id=openmp-building-histograms->OpenMP! Building histograms ğŸŒ‹</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>Find best splits by <em><strong>building histograms</strong></em></li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=openmp-building-histograms--1>OpenMP! Building histograms ğŸŒ‹</h1><pre><code class=language-python data-line-numbers=|1-4|6-8># sklearn/ensemble/_hist_gradient_boosting/histogram.pyx
with nogil:
    for feature_idx in prange(n_features, schedule='static'):
        self._compute_histogram_brute_single_feature(...)

for feature_idx in prange(n_features, schedule='static',
                          nogil=True):
    _subtract_histograms(feature_idx, ...)
</code></pre></section><section><h1 id=openmp-find-best-splits->OpenMP! Find best splits âœ‚ï¸</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li><em><strong>Find best splits</strong></em> by building histograms</li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=openmp-find-best-splits--1>OpenMP! Find best splits âœ‚ï¸</h1><pre><code class=language-py># sklearn/ensemble/_hist_gradient_boosting/splitting.pyx
for feature_idx in prange(n_features, schedule='static'):
    # For each feature, find best bin to split on
</code></pre></section><section><h1 id=openmp-splitting->OpenMP! Splitting âœ‚ï¸</h1><pre><code class=language-py># sklearn/ensemble/_hist_gradient_boosting/splitting.pyx
for thread_idx in prange(n_threads, schedule='static',
                         chunksize=1):
    # splits a partition of node
</code></pre></section><section><h1 id=openmp-update-gradients-and-hessians->OpenMP! Update gradients and hessians ğŸ”</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>Find best splits by building histograms</li><li>Add tree to predictors</li><li><em><strong>Update gradients and hessians</strong></em></li></ol></li></ol></section><section><h1 id=openmp-update-gradients-and-hessians--1>OpenMP! Update gradients and hessians ğŸ”</h1><p><code>least_squares</code></p><pre><code class=language-py># sklearn/ensemble/_hist_gradient_boosting/_loss.pyx
for i in prange(n_samples, schedule='static', nogil=True):
    gradients[i] = raw_predictions[i] - y_true[i]
</code></pre></section></section><section><section data-shortcode-section><h1 id=hyper-parameters->Hyper-parameters ğŸ““</h1></section><section><h1 id=hyper-parameters-bin-data->Hyper-parameters: Bin Data ğŸ—‘</h1><ol><li><em><strong>Bin data</strong></em></li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>Find best splits by building <em><strong>histograms</strong></em></li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=hyper-parameters-bin-data--1>Hyper-parameters: Bin Data ğŸ—‘</h1><p><code>max_bins=255</code></p><figure><img src=images/binning101.png height=500px></figure></section><section><h1 id=hyper-parameters-loss->Hyper-parameters: Loss ğŸ“‰</h1><ol><li>Bin data</li><li><em><strong>Make initial predictions (constant)</strong></em></li><li>Calculate <em><strong>gradients and hessians</strong></em></li><li>Grow Trees For Boosting<ol><li>Find best splits by building histograms</li><li>Add tree to predictors</li><li><em><strong>Update gradients and hessians</strong></em></li></ol></li></ol></section><section><h1 id=hyper-parameters-loss--1>Hyper-parameters: Loss ğŸ“‰</h1><ul><li><p><code>HistGradientBoostingRegressor</code></p><ul><li><code>loss=least_squares</code> (default)</li><li><code>least_absolute_deviation</code></li><li><code>poisson</code></li></ul></li><li><p><code>HistGradientBoostingClassifier</code></p><ul><li><code>loss=auto</code> (default)</li><li><code>binary_crossentropy</code></li><li><code>categorical_crossentropy</code></li></ul></li><li><p><code>l2_regularization=0</code></p></li></ul></section><section><h1 id=hyper-parameters-boosting->Hyper-parameters: Boosting ğŸ‚</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For <em><strong>Boosting</strong></em><ol><li>Find best splits by building histograms</li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=hyper-parameters-boosting--1>Hyper-parameters: Boosting ğŸ‚</h1><ul><li><code>learning_rate=0.1</code> ($\eta$)</li><li><code>max_iter=100</code></li></ul><p>$$
f(X) = C + \eta\sum_{m}^{\text{max_iter}}h_{m}(X)
$$</p></section><section><h1 id=hyper-parameters-boosting--2>Hyper-parameters: Boosting ğŸ‚</h1><figure><img src=images/boosting_p1.png height=600px></figure></section><section><h1 id=hyper-parameters-boosting--3>Hyper-parameters: Boosting ğŸ‚</h1><figure><img src=images/learning_rate_p1.png height=600px></figure></section><section><h1 id=hyper-parameters-grow-trees->Hyper-parameters: Grow Trees ğŸ„</h1><ol><li>Bin data</li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li><em><strong>Grow Trees</strong></em> For Boosting<ol><li>Find best splits by building histograms</li><li>Add tree to predictors</li><li>Update gradients and hessians</li></ol></li></ol></section><section><h1 id=hyper-parameters-grow-trees--1>Hyper-parameters: Grow Trees ğŸ„</h1><ul><li><code>max_leaf_nodes=31</code></li><li><code>max_depth=None</code></li><li><code>min_samples_leaf=20</code></li></ul></section><section><h1 id=hyper-parameters-grow-trees--2>Hyper-parameters: Grow Trees ğŸ„</h1><figure><img src=images/max_leaf_nodes_p1.png height=600px></figure></section><section><h1 id=hyper-parameters-grow-trees--3>Hyper-parameters: Grow Trees ğŸ„</h1><figure><img src=images/max_depth_p1.png height=600px></figure></section><section><h1 id=hyper-parameters-early-stopping->Hyper-parameters: Early Stopping ğŸ›‘</h1><ol><li>Bin data</li><li><em><strong>Split into a validation dataset</strong></em></li><li>Make initial predictions (constant)</li><li>Calculate gradients and hessians</li><li>Grow Trees For Boosting<ol><li>&mldr;</li><li><em><strong>Stop if early stop condition is true</strong></em></li></ol></li></ol></section><section><h1 id=hyper-parameters-early-stopping--1>Hyper-parameters: Early Stopping ğŸ›‘</h1><ul><li><code>early_stopping='auto'</code> (enabled if <code>n_samples>10_000</code>)</li><li><code>scoring='loss'</code></li><li><code>validation_fraction=0.1</code></li><li><code>n_iter_no_change=10</code></li><li><code>tol=1e-7</code></li></ul></section><section><h1 id=hyper-parameters-early-stopping--2>Hyper-parameters: Early Stopping ğŸ›‘</h1><figure><img src=images/early_stopping_p1.png height=600px></figure></section><section><h1 id=hyper-parameters-misc->Hyper-parameters: Misc ğŸ</h1><ul><li><code>verbose=0</code></li><li><code>random_state=None</code></li><li><code>export OMP_NUM_THREADS=8</code></li></ul></section></section><section><section data-shortcode-section><h1 id=recently-added-features>Recently Added Features</h1><ul><li>Missing values (0.22)</li><li>Monotonic constraints (0.23)</li><li>Poisson loss (0.23)</li><li>Categorical features (0.24)</li></ul></section><section><h1 id=missing-values-022>Missing Values (0.22)</h1><pre><code class=language-python>from sklearn.experimental import enable_hist_gradient_boosting  # noqa
from sklearn.ensemble import HistGradientBoostingClassifier
import numpy as np

X = np.array([0, 1, 2, np.nan]).reshape(-1, 1)
y = [0, 0, 1, 1]

gbdt = HistGradientBoostingClassifier(min_samples_leaf=1).fit(X, y)
gbdt.predict(X)
# [0 0 1 1]
</code></pre></section><section><h1 id=monotonic-constraints-023>Monotonic Constraints (0.23)</h1><pre><code class=language-python>from sklearn.experimental import enable_hist_gradient_boosting  # noqa
from sklearn.ensemble import HistGradientBoostingRegressor

X, y = ...

gbdt_no_cst = HistGradientBoostingRegressor().fit(X, y)
gbdt_cst = HistGradientBoostingRegressor(monotonic_cst=[1, 0]).fit(X, y)
</code></pre></section><section><h1 id=monotonic-constraints-023-1>Monotonic Constraints (0.23)</h1><pre><code class=language-python>from sklearn.inspection import plot_partial_dependence

disp = plot_partial_dependence(
    gbdt_no_cst, X, features=[0], feature_names=['feature 0'], line_kw={...})
plot_partial_dependence(gbdt_cst, X, features=[0], line_kw={...}, ax=disp.axes_)
</code></pre><figure><img src=images/monotonic_cst.png height=450px></figure></section><section><h1 id=poisson-loss-023>Poisson Loss (0.23)</h1><pre><code class=language-python>hist_poisson = HistGradientBoostingRegressor(loss='poisson')
</code></pre><figure><img src=images/poisson_hist.png height=450px></figure></section><section><h1 id=categorical-features-024>Categorical Features (0.24)</h1><p>From <a href=https://scikit-learn.org/dev/auto_examples/ensemble/plot_gradient_boosting_categorical.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-categorical-py>categorical example</a></p><pre><code class=language-python>categorical_mask = ([True] * n_categorical_features +
                    [False] * n_numerical_features)
hist = HistGradientBoostingRegressor(categorical_features=categorical_mask)
</code></pre><figure><img src=images/categorical_features.png height=450px></figure></section><section data-shortcode-section><h1 id=compared-to-other-libraries>Compared to Other Libraries</h1></section><section><h1 id=xgboost>XGBoost</h1><pre><code class=language-bash>conda install -c conda-forge xgboost
</code></pre><pre><code class=language-python>from xgboost import XGBClassifier
xgb = XGBClassifier()
</code></pre><ul><li>GPU training</li><li>Networked parallel training</li><li>Sparse data</li></ul></section><section><h1 id=lightgbm>LightGBM</h1><pre><code class=language-bash>conda install -c conda-forge lightgbm
</code></pre><pre><code class=language-python>from lightgbm.sklearn import LGBMClassifier
lgbm = LGBMClassifier()
</code></pre><ul><li>GPU training</li><li>Networked parallel training</li><li>Sparse data</li></ul></section><section><h1 id=catboost>CatBoost</h1><pre><code class=language-bash>conda install -c conda-forge catboost
</code></pre><pre><code class=language-python>from catboost.sklearn import CatBoostClassifier
catb = CatBoostClassifier()
</code></pre><ul><li>Focus on categorical features</li><li>Bagged and smoothed target encoding for categorical features</li><li>Symmetric trees</li><li>GPU training</li><li>Tooling</li></ul></section><section><h1 id=benchmark->Benchmark ğŸš€</h1><h2 id=higgs-boson>HIGGS Boson</h2><ul><li>8800000 samples</li><li>28 features</li><li>binary classification (1 for signal, 0 for background)</li></ul></section><section><h2 id=current-benchmark-results>Current Benchmark Results</h2><div class=g><div class=g-1><table><thead><tr><th>library</th><th>time</th><th>roc auc</th><th>accuracy</th></tr></thead><tbody><tr><td>sklearn</td><td>66s</td><td>0.8126</td><td>0.7325</td></tr><tr><td>lightgbm</td><td>42s</td><td>0.8125</td><td>0.7323</td></tr><tr><td>xgboost</td><td>45s</td><td>0.8124</td><td>0.7325</td></tr><tr><td>catboost</td><td>90s</td><td>0.8008</td><td>0.7223</td></tr></tbody></table></div><div class=g-1><h3 id=versions>Versions</h3><ul><li><code>xgboost=1.3.0.post0</code></li><li><code>lightgbm=3.1.1</code></li><li><code>catboost=0.24.3</code></li></ul></div></div></section></section><section><h1 id=conclusion>Conclusion</h1><div class="g g-middle"><div class=g-1><h2 id=future-work>Future Work</h2><ul><li>Sparse Data</li><li>Improve performance when compared to other frameworks.</li><li>Better way to pass feature-aligned metadata to estimators in a pipeline.</li></ul><p>Learn more about <a href=https://scikit-learn.org/stable/modules/ensemble.html#histogram-based-gradient-boosting>Histogram-Based Gradient Boosting</a></p><pre><code class=language-bash>pip install scikit-learn==0.24.0rc1
</code></pre></div><div class=g-1><p>Thomas J. Fan</p><p class=social-author>@thomasjpfan</p><a href=https://www.github.com/thomasjpfan target=_blank><span class="icon icon-github icon-left"></span></a><a href=https://www.twitter.com/thomasjpfan target=_blank><span class="icon icon-twitter"></span></a><div></div><a class=this-talk-link , href=https://github.com/thomasjpfan/2020-richmond-ds-meetup-gradient-boosting target=_blank>This talk on Github: thomasjpfan/2020-richmond-ds-meetup-gradient-boosting</a></div></div></section></div></div><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-hugo/object-assign.js></script><a href=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/css/print/ id=print-location style=display:none></a><script type=text/javascript>var printLocationElement=document.getElementById('print-location');var link=document.createElement('link');link.rel='stylesheet';link.type='text/css';link.href=printLocationElement.href+(window.location.search.match(/print-pdf/gi)?'pdf.css':'paper.css');document.getElementsByTagName('head')[0].appendChild(link);</script><script type=application/json id=reveal-hugo-site-params>{"custom_theme":"custom-theme.scss","custom_theme_compile":true,"height":720,"highlight_theme":"monokai-sublime","transition_speed":"fast","width":1280}</script><script type=application/json id=reveal-hugo-page-params>null</script><script src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/js/reveal.js></script><script type=text/javascript>function camelize(map){if(map){Object.keys(map).forEach(function(k){newK=k.replace(/(\_\w)/g,function(m){return m[1].toUpperCase()});if(newK!=k){map[newK]=map[k];delete map[k];}});}
return map;}
var revealHugoDefaults={center:true,controls:true,history:true,progress:true,transition:"slide"};var revealHugoSiteParams=JSON.parse(document.getElementById('reveal-hugo-site-params').innerHTML);var revealHugoPageParams=JSON.parse(document.getElementById('reveal-hugo-page-params').innerHTML);var options=Object.assign({},camelize(revealHugoDefaults),camelize(revealHugoSiteParams),camelize(revealHugoPageParams));Reveal.initialize(options);</script><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/plugin/markdown/marked.js></script><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/plugin/markdown/markdown.js></script><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/plugin/highlight/highlight.js></script><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/plugin/zoom-js/zoom.js></script><script type=text/javascript src=/2020-richmond-ds-meetup-gradient-boosting/reveal-js/plugin/notes/notes.js></script><script>MathJax={tex:{inlineMath:[['$','$'],['\\(','\\)']]},svg:{fontCache:'global'}};</script><script type=text/javascript id=MathJax-script async src=https://thomasjpfan.github.io/2020-richmond-ds-meetup-gradient-boosting//javascript/tex-svg.js></script><link rel="shortcut icon" href=https://thomasjpfan.github.io/2020-richmond-ds-meetup-gradient-boosting//favicon.ico></body></html>